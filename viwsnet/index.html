<html class="gr__richzhang_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script src="./The Unreasonable Effectiveness of Deep Networks as a Perceptual Metric_files/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

  .responsive-video {
    width: 100%;
    height: auto;
  }

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 0px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style> 
		<title>Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation</title>
  </head>

  <body data-gr-c-s-loaded="true">
    <br>
          <center>
          	<span style="font-size:32px">Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation</span><br>
	  		  
	  		  <br>

	  		  <table align="center" width="1000px">
	  			  <tbody><tr>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Yijun Yang<sup>1,2</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="200px">
	  					<center>
	  						<span style="font-size:22px">Angelica I. Aviles-Rivero<sup>3</sup></span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Huazhu Fu<sup>4</sup></span>
		  		  		</center>
		  		  	  </td>
				<br>
	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Ye Liu<sup>5</sup></span>
		  		  		</center>
		  		  	  </td>
					   </tr></tbody>

			  	<table align="center" width="500px">
	  			  <tbody><tr>
			  	              <td align="center" width="200px">
	  	        <center>
	  						<span style="font-size:22px">Weiming Wang<sup>6</sup></span>
		  		  		</center>
		  		  	  </td>
	  			 
	  			  				<td align="center" width="200px">
	  			  	<center>
	  						<span style="font-size:22px">Lei Zhu<sup>1,2</sup></span>
		  		  		</center>
		  		  	  </td>
<!-- 	  	              <td align="center" width="150px">
	  					<center>
	  						<span style="font-size:22px">Angelica I Aviles-Rivero<sup>1</sup></span>
		  		  		</center>
		  		  	  </td> -->
					  
	  			  </tr></tbody>
			  	</table>

					<br>

			  	<table align="center" width="850px">
	  			  <tbody><tr>
	  			  				<td align="center" width="120px">
	  			  	<center>
	  						<span style="font-size:18px"><sup>1</sup>HKUST (GZ)</span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="120px">
	  	        <center>
	  						<span style="font-size:18px">&nbsp&nbsp&nbsp&nbsp&nbsp<sup>2</sup>HKUST</span>
		  		  		</center>
		  		  	  </td>
	  	              <td align="center" width="120px">
	  					<center>
	  						<span style="font-size:18px"><sup>3</sup>University of Cambridge</span>
		  		  		</center>
		  		  	  </td>
		  		  	  		<td align="center" width="120px">
	  					<center>
	  						<span style="font-size:18px"><sup>4</sup>A*STAR</span>
		  		  		</center>
		  		  	  </td>
	  			  </tr></tbody>
			  	</table>
				  <table align="center" width="850px">
				  	  <tbody><tr>
					  <td align="center" width="40px">
	  					<center>
	  						<span style="font-size:18px"><sup>5</sup>Tianjin University</span>
		  		  		</center>
		  		  	  </td>
					  <td align="center" width="40px">
	  					<center>
	  						<span style="font-size:18px"><sup>6</sup>Hong Kong Metropolitan University</span>
		  		  		</center>
		  		  	  </td>
	  			  </tr></tbody>
			  	</table>

          <br>

	  		  <table align="center" width="1000px">
	  			  <tbody><tr>
	  	              <td width="50px">
	  					<center>
	  	              <img class="rounded" src="./figures/visualization_comparison.png" width="850px">
							</center>
		  	            </td>
		  	           </tr>
		  	    </tbody>
	  	    </table>

	  		  <br><br><hr>

  		  	<center><h1>Abstract</h1></center>
	  		  	<table align="center" width="900px">
	  					<tbody>
								<tr><td width="900px"><left>
		  						Although convolutional neural networks (CNNs) have been proposed to remove adverse weather conditions in single images using a single set of pre-trained weights, they fail to restore weather videos due to the absence of temporal information. Furthermore, existing methods for removing adverse weather conditions (e.g., rain, fog, and snow) from videos can only handle one type of adverse weather. In this work, we propose the first framework for restoring videos from all adverse weather conditions by developing a video adverse-weather-component suppression network (ViWS-Net). To achieve this, we first devise a weather-agnostic video transformer encoder with multiple transformer stages. Moreover, we design a long short-term temporal modeling mechanism for weather messenger to early fuse input adjacent video frames and learn weather-specific information. We further introduce a weather discriminator with gradient reversion, to maintain the weather-invariant common information and suppress the weather-specific information in pixel features, by adversarially predicting weather types. Finally, we develop a messenger-driven video transformer decoder to retrieve the residual weather-specific feature, which is spatiotemporally aggregated with hierarchical pixel features and refined to predict the clean target frame of input videos. Experimental results, on benchmark datasets and real-world weather videos, demonstrate that our ViWS-Net outperforms current state-of-the-art methods in terms of restoring videos degraded by any weather condition.
								</left></td></tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Overview of Network Architecture</h1></center>
			  		<table align="center" width="900px">
			  			<tbody>
			  			  	<tr>
			  			  		<td align="center"><img class="round" style="width:900px" src="./figures/framework.png"></td>	
						  		</tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Long Short-term Temporal Modeling</h1></center>
			  		<table align="center" width="800px">
			  			<tbody>
			  			  	<tr>
			  			  		<td align="center"><img class="round" style="width:800px" src="./figures/method_temporal.png"></td>	
						  		</tr>
							</tbody>
						</table>

					<br><br><hr>

					<center><h1>Video Adverse Weather Removal Results</h1></center>
						<p align="center">
							<!-- <iframe width="960" height="540" src="./videos/supplementary_1.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center" alt="ViWS-Net">
							</iframe> -->
							<video class="responsive-video" controls controlsList="nodownload" preload="metadata">
  						<source src="./videos/supplementary_1.mp4" type="video/mp4">
  						Your browser does not support the video tag.
							</video>
						</p>
<!-- 						<p align="center">
							<!-- <iframe width="960" height="540" src="./videos/supplementary_2.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center" alt="ViWS-Net">
							</iframe> -->
							<video class="responsive-video" controls controlsList="nodownload" preload="metadata">
  						<source src="./videos/supplementary_2.mp4" type="video/mp4">
  						Your browser does not support the video tag.
							</video>
						</p>						 -->
					<br><br><hr>

	  		  <center><h1>Paper and Code</h1></center>
	  		  <table align="center" width="650px">
	  			  <tbody><tr>
					  <td><a href=""><img class="layered-paper-big" style="height:175px" src="./figures/paper.png"></a></td>
					  <td><span style="font-size:12pt">Yijun Yang, Angelica I. Aviles-Rivero, Huazhu Fu, Ye Liu, <br>Weiming Wang, Lei Zhu.</span><br><br>
					  <b><span style="font-size:12pt">Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation.</span></b><br><br>
					  <span style="font-size:12pt">International Conference on Computer Vision (<b><font color="#FF0000">ICCV</font></b>), 2023.<br><br>
					  <a href="https://arxiv.org/abs/2211.06885" target="_blank">[arxiv]</a> 
					  <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:LMpZIn_iN7UJ:scholar.google.com/&output=citation&scisdr=CgUCEWi3ENfhkiZBiag:AAGBfm0AAAAAY7xHkahg-KlMznumBhg76NsUxStYwXU4&scisig=AAGBfm0AAAAAY7xHkf3zHk45paZANDGkBKM3WfJrL3os&scisf=4&ct=citation&cd=-1&hl=en" target="_blank">[bibtex]</a> 
					  <a href="https://github.com/lihaoliu-cambridge/viwsnet" target="_blank">[code]</a> 
					  </td>
	  	      </tr></tbody>
	  	    </table>

			  	<br><br><hr>

	  			<center><h1>Acknowledgments</h1></center>
  		  	<table align="center" width="900px">
  					<tbody>
							<tr><td width="900px"><left>
This work was supported by the Guangzhou Municipal Science and Technology Project (Grant No. 2023A03J0671), National Natural Science Foundation of China (Grant No. 61902275), and Hong Kong Metropolitan University Research Grant (No. RD/2021/09).							</left></td></tr>
						</tbody>
					</table>
		<br><br><hr>

				  	  			<center><h1>Citation</h1></center>
  		  	<table align="center" width="900px">
  					<tbody>
							<tr><td width="900px"><left>
	
<pre>
		@inproceedings{yang2023video,
  			title={Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation},
  			author={Yang, Yijun and Aviles-Rivero, Angelica and Fu, Huazhu and Liu, Ye and Wang, Weiming and Zhu, Lei},
  			booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  			year={2023}
		}
</pre>							
							</left></td></tr>
						</tbody>
					</table>

				  
<!-- Global site tag (gtag.js) - Google Analytics -->



</body></html>
